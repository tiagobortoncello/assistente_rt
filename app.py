# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import nltk
import json
import requests
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from collections import defaultdict
import os

# O download de stopwords √© necess√°rio para a parte de resumo (se o modelo precisar) e √© uma boa pr√°tica
nltk.download('stopwords', quiet=True)

# --- Configura√ß√£o do Gemini API para gera√ß√£o de termos ---
# ATEN√á√ÉO: A chave de API agora √© lida de uma vari√°vel de ambiente por seguran√ßa
API_KEY = os.getenv("API_KEY")

if not API_KEY:
    st.error("Erro: A chave de API n√£o foi encontrada. Por favor, configure a vari√°vel de ambiente 'API_KEY'.")
    st.stop()
    
API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent"

# --- Fun√ß√µes de Leitura e Processamento do Dicion√°rio ---
def carregar_dicionario_termos(filepath):
    """
    Carrega os termos de um arquivo de texto, com um termo por linha.
    Processa a hierarquia para criar uma lista plana e um mapa de rela√ß√µes.
    """
    termos_plano = []
    hierarquia_mapa = {}
    try:
        with open(filepath, 'r', encoding='utf-8') as file:
            for line in file:
                line = line.strip()
                if not line:
                    continue
                
                partes = [p.strip() for p in line.split('>')][::-1]
                
                # Adiciona o termo mais espec√≠fico √† lista plana
                termo_especifico = partes[0]
                termos_plano.append(termo_especifico)
                
                # Mapeia o termo espec√≠fico ao seu gen√©rico (se houver)
                if len(partes) > 1:
                    termo_generico = partes[1]
                    hierarquia_mapa[termo_especifico] = termo_generico
                    # Adiciona o termo gen√©rico √† lista plana se ainda n√£o estiver l√°
                    if termo_generico not in termos_plano:
                        termos_plano.append(termo_generico)
                        
    except FileNotFoundError:
        st.error(f"Erro: O arquivo '{filepath}' n√£o foi encontrado. Por favor, crie o arquivo e adicione seus termos, um por linha.")
        return [], {}
        
    return termos_plano, hierarquia_mapa

# Carrega o dicion√°rio de termos do arquivo e o mapa de hierarquia
TERMOS_DICIONARIO, HIERARQUIA_MAPA = carregar_dicionario_termos("dicionario_termos.txt")

def aplicar_logica_hierarquia(termos_sugeridos, hierarquia_mapa):
    """
    Filtra a lista de termos sugeridos para remover gen√©ricos se um termo
    mais espec√≠fico da mesma hierarquia estiver presente.
    """
    termos_sugeridos_set = {t.strip() for t in termos_sugeridos}
    termos_finais = set(termos_sugeridos_set)

    for termo in termos_sugeridos_set:
        termo_atual = termo
        while termo_atual in hierarquia_mapa:
            termo_generico = hierarquia_mapa[termo_atual]
            if termo_generico in termos_finais:
                termos_finais.discard(termo_generico)
            termo_atual = termo_generico
    
    return list(termos_finais)

# --- Fun√ß√µes de Gera√ß√£o de Termos e Resumos ---

def gerar_termos_llm(texto):
    """
    Gera termos de indexa√ß√£o usando um LLM (Gemini Flash), limitando
    as sugest√µes ao dicion√°rio predefinido.
    """
    # Verifica se o dicion√°rio est√° carregado
    if not TERMOS_DICIONARIO:
        return []

    system_instruction = {
        "parts": [{
            "text": f"""
                Voc√™ √© um indexador de documentos legislativos altamente experiente.
                Sua tarefa √© analisar o texto de uma proposi√ß√£o e gerar uma lista de 5 a 8 termos de indexa√ß√£o que capturem os t√≥picos principais e o escopo do documento.
                √â absolutamente crucial que os termos sugeridos sejam escolhidos EXCLUSIVAMENTE da seguinte lista:
                {', '.join(TERMOS_DICIONARIO)}
                Se o texto n√£o se encaixar em nenhum termo da lista, retorne uma lista vazia.
                """
        }]
    }

    payload = {
        "contents": [{"parts": [{"text": texto}]}],
        "systemInstruction": system_instruction,
        "generationConfig": {
            "responseMimeType": "application/json",
            "responseSchema": {
                "type": "ARRAY",
                "items": {"type": "STRING"}
            }
        }
    }

    try:
        response = requests.post(
            f"{API_URL}?key={API_KEY}",
            headers={'Content-Type': 'application/json'},
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        json_string = result['candidates'][0]['content']['parts'][0]['text']
        termos_gerados = json.loads(json_string)
        
        # Filtra os termos gerados para garantir que eles est√£o no dicion√°rio
        dicionario_set = {t.strip().lower() for t in TERMOS_DICIONARIO}
        termos_filtrados = [
            t for t in termos_gerados if t.strip().lower() in dicionario_set
        ]

        # Aplica a l√≥gica de hierarquia para remover gen√©ricos desnecess√°rios
        termos_finais = aplicar_logica_hierarquia(termos_filtrados, HIERARQUIA_MAPA)
        
        return [t.strip() for t in termos_finais if t]
    except requests.exceptions.RequestException as e:
        st.error(f"Erro na comunica√ß√£o com a API: {e}")
        return []
    except (json.JSONDecodeError, KeyError) as e:
        st.error(f"Erro ao processar a resposta da API: {e}")
        return []

# Modelo de resumo em portugu√™s (mantido do seu c√≥digo original)
MODEL_NAME = "rhaymison/flan-t5-portuguese-small-summarization"

@st.cache_resource
def load_summarizer():
    """Carrega o modelo de resumo com cache para evitar recarregar."""
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
    summarizer_pipeline = pipeline(
        "summarization",
        model=model,
        tokenizer=tokenizer,
        device=-1 # for√ßa CPU
    )
    return summarizer_pipeline

summarizer = load_summarizer()

def gerar_resumo(texto, tipo):
    """Gera um resumo do texto no estilo legislativo com regras fixas."""
    regras_adicionais = "Use linguagem formal, evite g√≠rias e mantenha um tom objetivo e neutro. O resumo deve usar verbos na terceira pessoa do singular."
    
    prompt_completo = f"Resuma o seguinte {tipo} em estilo legislativo, texto corrido, objetivo, sem perder informa√ß√µes essenciais:\n\n{texto}\n\nRegras adicionais: {regras_adicionais}"
    
    resumo = summarizer(prompt_completo, max_length=120, min_length=40, do_sample=False)
    return resumo[0]["summary_text"]

# --- Interface Streamlit ---

st.set_page_config(page_title="Assistente de indexa√ß√£o e resumos", layout="wide")

st.title("Assistente de indexa√ß√£o e resumos")
st.subheader("Ger√™ncia de Informa√ß√£o Legislativa ‚Äì GIL/GDI")
st.write("---")

tipo = st.selectbox("Escolha o tipo de documento:", ["Proposi√ß√£o", "Requerimento"])
texto_input = st.text_area("Cole a ementa ou texto aqui:", height=200)

if st.button("Gerar sugest√µes e resumo"):
    if texto_input.strip() == "":
        st.warning("Por favor, insira um texto para an√°lise.")
    else:
        # Sugest√£o de termos usando o novo m√©todo com LLM
        st.markdown("### üîë Termos sugeridos")
        
        # Exibe um spinner enquanto a API est√° trabalhando
        with st.spinner('Gerando termos...'):
            termos_sugeridos = gerar_termos_llm(texto_input)
            st.write(", ".join(termos_sugeridos) if termos_sugeridos else "Nenhum termo sugerido.")

        # Resumo
        st.markdown("### üìù Resumo gerado")
        with st.spinner('Gerando resumo...'):
            resumo = gerar_resumo(texto_input, tipo)
            st.success(resumo)
